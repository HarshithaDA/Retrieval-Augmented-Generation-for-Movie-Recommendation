{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMvfdfS71DfO"
      },
      "source": [
        "# Building a RAG System With Google's Gemma, Hugging Face and MongoDB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv3agCpq1i36"
      },
      "source": [
        "https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NctbiP32KGG"
      },
      "source": [
        "Installing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfCOIxCe0xzx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pandas pymongo sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmim9Dab1z4b"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XksMd19117C"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVnK0sUK162d"
      },
      "source": [
        "Loading dataset:\n",
        "\n",
        "https://huggingface.co/datasets/MongoDB/embedded_movies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "417zzsFr2CbQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8k8Sq_V8i7Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_df = pd.read_json(\"https://huggingface.co/datasets/MongoDB/embedded_movies/resolve/main/sample_mflix.embedded_movies.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-0RFss44mys"
      },
      "outputs": [],
      "source": [
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-MtBiUt8kog"
      },
      "outputs": [],
      "source": [
        "# Remove data point where plot column is missing\n",
        "dataset_df = dataset_df.dropna(subset=['fullplot'])\n",
        "print(\"\\nNumber of missing values in each column after removal:\")\n",
        "print(dataset_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wi5a9sh91rB"
      },
      "outputs": [],
      "source": [
        "# Remove the plot_embedding from each data point in the dataset as we are going to create new embeddings with an open-source embedding model from Hugging Face: gte-large\n",
        "dataset_df = dataset_df.drop(columns=['plot_embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATobFcta5jXg"
      },
      "source": [
        "Generating Embeddings\n",
        "\n",
        "\n",
        "Embedding models convert high-dimensional data such as text, audio, and images into a lower-dimensional numerical representation that captures the input data's semantics and context.\n",
        "\n",
        "This embedding representation of data can be used to conduct semantic searches based on the positions and proximity of embeddings to each other within a vector space.\n",
        "\n",
        "\n",
        "The embedding model used in the RAG system is the Generate Text Embedding (GTE) model, based on the BERT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/mteb/leaderboard  \n",
        "\n",
        "\n",
        "Retrieval"
      ],
      "metadata": {
        "id": "Uk0D3u4avQ8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRlyqXR65kzU"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# https://huggingface.co/thenlper/gte-large\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezh-FH4kR3p0"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text: str) -> list[float]:\n",
        "      if not text.strip():\n",
        "         print(\"Attempted to get embedding for empty text.\")\n",
        "         return []\n",
        "      embedding = embedding_model.encode(text)\n",
        "      return embedding.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8NcwckLR969"
      },
      "outputs": [],
      "source": [
        "dataset_df[\"embedding\"] = dataset_df[\"fullplot\"].apply(get_embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "f_cGsZgtvmtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cutBMWK9Sht8"
      },
      "source": [
        "we now have a complete dataset with embeddings that can be ingested into a vector database, like MongoDB, where vector search operations can be performed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mongo DB connection"
      ],
      "metadata": {
        "id": "rE-1McCMrYss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "PnTONcSJsjBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_mongo_client(mongo_uri):\n",
        "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "    try:\n",
        "        client = pymongo.MongoClient(mongo_uri)\n",
        "        print(\"Connection to MongoDB successful\")\n",
        "        return client\n",
        "    except pymongo.errors.ConnectionFailure as e:\n",
        "        print(f\"Connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "mongo_uri = userdata.get(\"MONGO_URI\")\n",
        "if not mongo_uri:\n",
        "    print(\"MONGO_URI not set in environment variables\")\n",
        "\n",
        "mongo_client = get_mongo_client(mongo_uri)\n",
        "\n",
        "# Ingest data into MongoDB\n",
        "db = mongo_client[\"movies\"]\n",
        "collection = db[\"movie_collection_2\"]"
      ],
      "metadata": {
        "id": "qwhi0thUpVIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete any existing records in the collection\n",
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "MmbIkhgorWf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Ingestion and Vector Search\n"
      ],
      "metadata": {
        "id": "NindpB-Ns3VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataset into list of disctionary, each row in dataframe is converted into a single record"
      ],
      "metadata": {
        "id": "0tsIgcMDwZQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = dataset_df.to_dict('records')\n",
        "collection.insert_many(documents)\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ],
      "metadata": {
        "id": "HGozo8EaqSfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(user_query, collection):\n",
        "    \"\"\"\n",
        "    Perform a vector search in the MongoDB collection based on the user query.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "    collection (MongoCollection): The MongoDB collection to search.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matching documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for the user query\n",
        "    query_embedding = get_embedding(user_query)\n",
        "    print(f\"Embedding shape: {len(query_embedding)}\")\n",
        "\n",
        "\n",
        "    if query_embedding is None:\n",
        "        return \"Invalid query or embedding generation failed.\"\n",
        "\n",
        "    # Define the vector search pipeline\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
        "                \"limit\": 4,  # Return top 4 matches\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,  # Exclude the _id field\n",
        "                \"fullplot\": 1,  # Include the plot field\n",
        "                \"title\": 1,  # Include the title field\n",
        "                \"genres\": 1,  # Include the genres field\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
        "            }\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Execute the search\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)\n"
      ],
      "metadata": {
        "id": "q_iit9NwqhRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling user queries and loading Gemma"
      ],
      "metadata": {
        "id": "63SmBSaNqkaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_result(query, collection):\n",
        "\n",
        "    get_knowledge = vector_search(query, collection)\n",
        "\n",
        "    search_result = \"\"\n",
        "    for result in get_knowledge:\n",
        "        search_result += f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('fullplot', 'N/A')}\\n\"\n",
        "\n",
        "    return search_result"
      ],
      "metadata": {
        "id": "AYxrz0H_qnWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conduct query with retrieval of sources\n",
        "query = \"What is the best romantic movie to watch and why?\"\n",
        "source_information = get_search_result(query, collection)\n",
        "combined_information = f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
        "print(combined_information)"
      ],
      "metadata": {
        "id": "s9W3bWLuxMMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collection.count_documents({}))\n",
        "print(collection.find_one())\n"
      ],
      "metadata": {
        "id": "35dO4M0Oxwrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "# CPU Enabled uncomment below\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "# GPU Enabled use below\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "mNb2SRPlqpiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(**input_ids, max_new_tokens=500)\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "lgFRGrL9qsNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}